# -*- coding: utf-8 -*-
"""ModelForQA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S36TTsG4j9ezOWJgHEeSE0hMvP1HWBHz
"""

# !pip install -U transformers datasets pandas scikit-learn torch accelerate
# !pip install transformers[torch]

import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset, Dataset, DatasetDict

from transformers import pipeline

pipe = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Load model directly
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

check_point = "deepset/roberta-base-squad2"
tokenizer = AutoTokenizer.from_pretrained(check_point)
model = AutoModelForQuestionAnswering.from_pretrained(check_point)

# Data preprocessing
import pandas as pd
from sklearn.model_selection import train_test_split

KNOWLEDGEBASE_DS_PATH = r"/content/knowledgebase.csv"  # TODO: REPLACE THE PATH IN YOUR PC!

knowledgebase_df = pd.read_csv(KNOWLEDGEBASE_DS_PATH)
knowledgebase_df = knowledgebase_df.dropna(how='any')

# Data preprocessing
df_train, tmp_data = train_test_split(knowledgebase_df, test_size=0.2, random_state=42)
valid_df, test_df = train_test_split(tmp_data, test_size=0.5, random_state=42)

# Convert the datagrames into DataSet objects
ds_train = Dataset.from_pandas(df_train)
valid_ds = Dataset.from_pandas(valid_df)
test_ds = Dataset.from_pandas(test_df)

dataset = Dataset.from_pandas(knowledgebase_df)
dataset_dict = DatasetDict(
    {
        "train": ds_train,
        "valid": valid_ds,
        "test": test_ds
    }
)
print(dataset_dict)

from transformers import DataCollatorWithPadding


# Tokenize the data
def tokenize(row):
    return tokenizer(row['Question'], row['Answer'], padding='max_length', truncation=True, return_tensors="pt")


tokenized_dataset_dict = dataset_dict.map(tokenize, batched=True)  # Tokinize exactly
data_collector = DataCollatorWithPadding(tokenizer)

# Example of the embedding representation of some sentence.
q = tokenized_dataset_dict['train']['Question'][10]
emb_q = tokenized_dataset_dict['train']['input_ids'][10][:len(q)]
print(f"The sentence: {q}")
print(f"The embedding: {emb_q}")

from transformers import TrainingArguments

training_args = TrainingArguments(
    "test-trainer")  # Before we initialize the Trainer, we need to define a training argument.

# Defining the Trainer before training the model
from transformers import Trainer

tariner = Trainer(model,
                  training_args,
                  train_dataset=tokenized_dataset_dict['train'],
                  eval_dataset=tokenized_dataset_dict['valid'],
                  data_collator=data_collector,
                  tokenizer=tokenizer)
tariner.train()

# from torch.utils.data import DataLoader

# batch_size = 5 # The default is 32, but we got a small dataset.
# # Setting up the dataloader
# train_dataloader = DataLoader(tokenized_dataset_dict['train'], batch_size=batch_size,shuffle =)

# optimizer = AdamW(model.parameters())
# epochs = 20 # TODO: Play with the epochs to get the best accuracy rate.
# for epcoh in range(epochs):
#   for batch in train_dataloader:
#     optimizer.zero_grad() # clear the gradients.
#     outputs = model(**batch) # retrain the batch
#     loss = outputs.loss # Extract the loss function output
#     loss.backward()
#     optimizer.step()
#   print(f"epcoh {epcoh} with loss {loss}")
